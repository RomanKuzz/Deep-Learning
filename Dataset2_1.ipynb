{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install scikit-learn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "\n",
    "# pip install tensorflow\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('database.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()\n",
    "\n",
    "# I will drop the columns that have more than 30% of missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['Depth Error', 'Depth Seismic Stations', 'Magnitude Error', 'Magnitude Seismic Stations', 'Azimuthal Gap', 'Horizontal Distance', 'Horizontal Error', 'Root Mean Square'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df  = df.dropna(subset=['Magnitude Type'])\n",
    "\n",
    "# removed the rows with missing values in the column Magnitude Type\n",
    "\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Latitude'].hist()\n",
    "\n",
    "# Latitude is a normal distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = df['Source'].unique()\n",
    "values\n",
    "\n",
    "value_counts = df['Source'].value_counts()\n",
    "value_counts\n",
    "\n",
    "df['Source'] = df['Source'].replace({'US': 1, 'ISCGEM': 0})\n",
    "\n",
    "df['Source'] = pd.to_numeric(df['Source'], errors='coerce')\n",
    "\n",
    "df = df.dropna(subset=['Source'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = df['Location Source'].unique()\n",
    "values\n",
    "\n",
    "value_counts = df['Location Source'].value_counts()\n",
    "value_counts\n",
    "\n",
    "df['Location Source'] = df['Location Source'].replace({'US': 1, 'ISCGEM': 0})\n",
    "\n",
    "df['Location Source'] = pd.to_numeric(df['Location Source'], errors='coerce')\n",
    "\n",
    "df = df.dropna(subset=['Location Source'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = df['Status'].unique()\n",
    "values\n",
    "\n",
    "value_counts = df['Status'].value_counts()\n",
    "value_counts\n",
    "\n",
    "df['Status'] = df['Status'].replace({'Automatic': 1, 'Reviewed': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values_source = df['ID'].unique()\n",
    "len(values_source)\n",
    "df = df.drop(['ID'], axis=1)\n",
    "\n",
    "# No need to keep the ID column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = df['Magnitude Type'].unique()\n",
    "df = df.drop(['Magnitude Type'], axis=1)\n",
    "df = df.drop(['Magnitude Source'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = df['Type'].unique()\n",
    "values\n",
    "\n",
    "value_counts = df['Type'].value_counts()\n",
    "value_counts\n",
    "\n",
    "df['Type'] = df['Type'].replace({'Earthquake': 1, 'Nuclear Explosion': 0})\n",
    "\n",
    "df['Type'] = pd.to_numeric(df['Type'], errors='coerce')\n",
    "\n",
    "df = df.dropna(subset=['Type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['Date', 'Time'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_data = df.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Check for NaN values\n",
    "non_numeric_values = numeric_data.isna().any()\n",
    "non_numeric_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.corr().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    " # everything else except the target variable\n",
    "X = df.drop('Latitude', axis=1)\n",
    "\n",
    "# have only the target variable here (dependent variable)\n",
    "y_temp = df['Latitude']\n",
    "\n",
    "# since we are doing classification, we have to process our target values with an encoder\n",
    "# and convert them into a categorical TensorFlow/Keras -format \n",
    "le = LabelEncoder()\n",
    "y_enc = le.fit_transform(y_temp)\n",
    "\n",
    "# Converting the label into a matrix form\n",
    "y = tf.keras.utils.to_categorical(y_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "# convert all continuous variables to integer,\n",
    "# and convert all negative numbers to 0\n",
    "X_cat = X.astype(int)\n",
    "X_cat = X_cat.clip(lower=0)\n",
    "\n",
    "# initialize chi2 and SelectKBest\n",
    "# Note: chi2 -test is a very common test\n",
    "# in statistics and quantitative analysis\n",
    "# basically it studies the data whether variables are related\n",
    "# or independent of each other\n",
    "chi_2_features = SelectKBest(chi2, k=len(X_cat.columns))\n",
    "\n",
    "# fit our data to the SelectKBest\n",
    "best_features = chi_2_features.fit(X_cat,y.astype(int))\n",
    "\n",
    "# use decimal format in table print later\n",
    "pd.options.display.float_format = '{:.2f}'.format\n",
    "\n",
    "# wrap it up, and show the results\n",
    "# the higher the score, the more effect that column has on price\n",
    "df_features = pd.DataFrame(best_features.scores_)\n",
    "df_columns = pd.DataFrame(X_cat.columns)\n",
    "f_scores = pd.concat([df_columns,df_features],axis=1)\n",
    "f_scores.columns = ['Features','Score']\n",
    "f_scores.sort_values(by='Score',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.30)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the categories into a list\n",
    "categories = list(np.unique(df['Latitude']))\n",
    "print(categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential(\n",
    "    [\n",
    "        layers.BatchNormalization(input_shape=(len(X.columns),)),\n",
    "        layers.Dense(16, activation=\"relu\", kernel_regularizer=keras.regularizers.l1(l=0.1)),\n",
    "        layers.Dense(32, activation=\"relu\"),\n",
    "        layers.Dropout(0.2),\n",
    "        layers.Dense(8, activation=\"relu\"),\n",
    "        layers.Dense(len(categories), activation=\"softmax\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "# compile the model, this time we use categorical crossentropy for loss -function\n",
    "# and we also measure the accuracy of our model in the metrics\n",
    "model.compile(optimizer=\"adam\", loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x=X_train, y=y_train, epochs=500, validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_df = pd.DataFrame(model.history.history)\n",
    "loss_df[['loss', 'val_loss']].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_df[['accuracy', 'val_accuracy']].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Test data evaluation:\")\n",
    "print(model.evaluate(X_test, y_test, verbose=0))\n",
    "print(\"\\nTrain data evaluation:\")\n",
    "print(model.evaluate(X_train, y_train, verbose=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = model.predict(X_test)\n",
    "test_predictions = np.argmax(test_predictions, axis=1)\n",
    "\n",
    "# convert also y-test -values with argmax\n",
    "y_test = np.argmax(y_test, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    " # the original heatmap without label names\n",
    "# sns.heatmap(confusion_matrix(y_test, test_predictions), annot=True, fmt='g')\n",
    "\n",
    "# you can also use the actual names for the categories\n",
    "sns.heatmap(confusion_matrix(y_test, test_predictions), xticklabels=categories, yticklabels=categories, annot=True, fmt='g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, test_predictions, target_names=categories))\n",
    "\n",
    "# get overall accuracy of the model and print it\n",
    "acc = accuracy_score(y_test, test_predictions)\n",
    "print(\"\\nModel overall accuracy: {:.2f}%\".format(acc * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score(y, model.predict(X), multi_class=\"ovr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tester_row = {\n",
    "    'Longitude': 145,\n",
    "    'Type': 1,\n",
    "    'Depth': 130,\n",
    "    'Magnitude': 6, \n",
    "    'Source': 0,\n",
    "    'Location Source': 0, \n",
    "    'Status': 1\n",
    "}\n",
    "\n",
    "# convert to pandas-format\n",
    "tester_row = pd.DataFrame([tester_row])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model.predict(tester_row)[0]\n",
    "\n",
    "print()\n",
    "print(f\"Estimated Latitude:\")\n",
    "print(f\"{round(float(result), 2)}\")\n",
    "print(\"----------------\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
