{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# we need this in later phases\n",
    "def activation_ReLu(value):\n",
    "    if value > 0:\n",
    "        return value\n",
    "    else: \n",
    "        return 0\n",
    "    \n",
    "# we need this in later phases\n",
    "def activation_ReLu_partial_derivative(value):\n",
    "    if value > 0:\n",
    "        return 1\n",
    "    else: \n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>bmi</th>\n",
       "      <th>children</th>\n",
       "      <th>smoker</th>\n",
       "      <th>region</th>\n",
       "      <th>charges</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19</td>\n",
       "      <td>female</td>\n",
       "      <td>27.900</td>\n",
       "      <td>0</td>\n",
       "      <td>yes</td>\n",
       "      <td>southwest</td>\n",
       "      <td>16884.92400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>18</td>\n",
       "      <td>male</td>\n",
       "      <td>33.770</td>\n",
       "      <td>1</td>\n",
       "      <td>no</td>\n",
       "      <td>southeast</td>\n",
       "      <td>1725.55230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>28</td>\n",
       "      <td>male</td>\n",
       "      <td>33.000</td>\n",
       "      <td>3</td>\n",
       "      <td>no</td>\n",
       "      <td>southeast</td>\n",
       "      <td>4449.46200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>33</td>\n",
       "      <td>male</td>\n",
       "      <td>22.705</td>\n",
       "      <td>0</td>\n",
       "      <td>no</td>\n",
       "      <td>northwest</td>\n",
       "      <td>21984.47061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>32</td>\n",
       "      <td>male</td>\n",
       "      <td>28.880</td>\n",
       "      <td>0</td>\n",
       "      <td>no</td>\n",
       "      <td>northwest</td>\n",
       "      <td>3866.85520</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age     sex     bmi  children smoker     region      charges\n",
       "0   19  female  27.900         0    yes  southwest  16884.92400\n",
       "1   18    male  33.770         1     no  southeast   1725.55230\n",
       "2   28    male  33.000         3     no  southeast   4449.46200\n",
       "3   33    male  22.705         0     no  northwest  21984.47061\n",
       "4   32    male  28.880         0     no  northwest   3866.85520"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('medical_insurance.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# since our neural network can only handle two variables + target variable\n",
    "# let's select age, bmi and charges\n",
    "# basically our x1, x2 and target is y (charges)\n",
    "df = df[[\"age\", \"bmi\", \"charges\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, loss: 0.04216958995368006\n",
      "Epoch: 2, loss: 0.03668692536297482\n",
      "Epoch: 3, loss: 0.03668584418001817\n",
      "Epoch: 4, loss: 0.036684773238985315\n",
      "Epoch: 5, loss: 0.036683712432889426\n",
      "Epoch: 6, loss: 0.03668266165607415\n",
      "Epoch: 7, loss: 0.03668162080419311\n",
      "Epoch: 8, loss: 0.03668058977419378\n",
      "Epoch: 9, loss: 0.036679568464298026\n",
      "Epoch: 10, loss: 0.03667855677398506\n",
      "Epoch: 11, loss: 0.036677554603973934\n",
      "Epoch: 12, loss: 0.036676561856205915\n",
      "Epoch: 13, loss: 0.036675578433827245\n",
      "Epoch: 14, loss: 0.036674604241172526\n",
      "Epoch: 15, loss: 0.03667363918374801\n",
      "Epoch: 16, loss: 0.0366726831682151\n",
      "Epoch: 17, loss: 0.036671736102373835\n",
      "Epoch: 18, loss: 0.03667079789514689\n",
      "Epoch: 19, loss: 0.03666986845656401\n",
      "Epoch: 20, loss: 0.03666894769774603\n",
      "Epoch: 21, loss: 0.03666803553088955\n",
      "Epoch: 22, loss: 0.036667131869251524\n",
      "Epoch: 23, loss: 0.03666623662713462\n",
      "Epoch: 24, loss: 0.03666534971987142\n",
      "Epoch: 25, loss: 0.03666447106381163\n",
      "Epoch: 26, loss: 0.036663600576305334\n",
      "Epoch: 27, loss: 0.036662738175690254\n",
      "Epoch: 28, loss: 0.03666188378127738\n",
      "Epoch: 29, loss: 0.036661037313337023\n",
      "Epoch: 30, loss: 0.03666019869308503\n",
      "Epoch: 31, loss: 0.036659367842669606\n",
      "Epoch: 32, loss: 0.03665854468515792\n",
      "Epoch: 33, loss: 0.036657729144522776\n",
      "Epoch: 34, loss: 0.03665692114563009\n",
      "Epoch: 35, loss: 0.0366561206142257\n",
      "Epoch: 36, loss: 0.03665532747692344\n",
      "Epoch: 37, loss: 0.036654541661192544\n",
      "Epoch: 38, loss: 0.0366537630953449\n",
      "Epoch: 39, loss: 0.03665299170852405\n",
      "Epoch: 40, loss: 0.036652227430692595\n",
      "Epoch: 41, loss: 0.036651470192620904\n",
      "Epoch: 42, loss: 0.036650719925875805\n",
      "Epoch: 43, loss: 0.03664997656280864\n",
      "Epoch: 44, loss: 0.03664924003654493\n",
      "Epoch: 45, loss: 0.03664851028097251\n",
      "Epoch: 46, loss: 0.03664778723073209\n",
      "Epoch: 47, loss: 0.03664707082120512\n",
      "Epoch: 48, loss: 0.03664636098850417\n",
      "Epoch: 49, loss: 0.03664565766946233\n",
      "Epoch: 50, loss: 0.036644960801623674\n",
      "Epoch: 51, loss: 0.03664427032323221\n",
      "Epoch: 52, loss: 0.03664358617322264\n",
      "Epoch: 53, loss: 0.03664290829121099\n",
      "Epoch: 54, loss: 0.03664223661748435\n",
      "Epoch: 55, loss: 0.036641571092991924\n",
      "Epoch: 56, loss: 0.0366409116593358\n",
      "Epoch: 57, loss: 0.03664025825876128\n",
      "Epoch: 58, loss: 0.03663961083414949\n",
      "Epoch: 59, loss: 0.03663896932900597\n",
      "Epoch: 60, loss: 0.036638333687454464\n",
      "Epoch: 61, loss: 0.036637703854227155\n",
      "Epoch: 62, loss: 0.036637079774656314\n",
      "Epoch: 63, loss: 0.03663646139466658\n",
      "Epoch: 64, loss: 0.03663584866076587\n",
      "Epoch: 65, loss: 0.03663524152003855\n",
      "Epoch: 66, loss: 0.03663463992013656\n",
      "Epoch: 67, loss: 0.036634043809271925\n",
      "Epoch: 68, loss: 0.03663345313620956\n",
      "Epoch: 69, loss: 0.03663286785025898\n",
      "Epoch: 70, loss: 0.03663228790126781\n",
      "Epoch: 71, loss: 0.036631713239613554\n",
      "Epoch: 72, loss: 0.03663114381619741\n",
      "Epoch: 73, loss: 0.036630579582436346\n",
      "Epoch: 74, loss: 0.03663002049025669\n",
      "Epoch: 75, loss: 0.03662946649208687\n",
      "Epoch: 76, loss: 0.03662891754085157\n",
      "Epoch: 77, loss: 0.036628373589964076\n",
      "Epoch: 78, loss: 0.036627834593320585\n",
      "Epoch: 79, loss: 0.03662730050529329\n",
      "Epoch: 80, loss: 0.036626771280724044\n",
      "Epoch: 81, loss: 0.03662624687491903\n",
      "Epoch: 82, loss: 0.036625727243641444\n",
      "Epoch: 83, loss: 0.03662521234310681\n",
      "Epoch: 84, loss: 0.03662470212997575\n",
      "Epoch: 85, loss: 0.03662419656134907\n",
      "Epoch: 86, loss: 0.03662369559476183\n",
      "Epoch: 87, loss: 0.03662319918817798\n",
      "Epoch: 88, loss: 0.03662270729998406\n",
      "Epoch: 89, loss: 0.03662221988898483\n",
      "Epoch: 90, loss: 0.03662173691439715\n",
      "Epoch: 91, loss: 0.036621258335844745\n",
      "Epoch: 92, loss: 0.036620784113353844\n",
      "Epoch: 93, loss: 0.03662031420734694\n",
      "Epoch: 94, loss: 0.03661984857863854\n",
      "Epoch: 95, loss: 0.03661938718843028\n",
      "Epoch: 96, loss: 0.03661892999830559\n",
      "Epoch: 97, loss: 0.03661847697022517\n",
      "Epoch: 98, loss: 0.03661802806652254\n",
      "Epoch: 99, loss: 0.036617583249898875\n",
      "Epoch: 100, loss: 0.0366171424834192\n",
      "Epoch: 101, loss: 0.036616705730507015\n",
      "Epoch: 102, loss: 0.036616272954940556\n",
      "Epoch: 103, loss: 0.03661584412084883\n",
      "Epoch: 104, loss: 0.03661541919270611\n",
      "Epoch: 105, loss: 0.036614998135328716\n",
      "Epoch: 106, loss: 0.036614580913870945\n",
      "Epoch: 107, loss: 0.036614167493820154\n",
      "Epoch: 108, loss: 0.03661375784099381\n",
      "Epoch: 109, loss: 0.03661335192153491\n",
      "Epoch: 110, loss: 0.036612949701908176\n",
      "Epoch: 111, loss: 0.03661255114889648\n",
      "Epoch: 112, loss: 0.03661215622959704\n",
      "Epoch: 113, loss: 0.03661176491141753\n",
      "Epoch: 114, loss: 0.03661137716207264\n",
      "Epoch: 115, loss: 0.0366109929495805\n",
      "Epoch: 116, loss: 0.03661061224225899\n",
      "Epoch: 117, loss: 0.036610235008722715\n",
      "Epoch: 118, loss: 0.03660986121787901\n",
      "Epoch: 119, loss: 0.03660949083892504\n",
      "Epoch: 120, loss: 0.036609123841344106\n",
      "Epoch: 121, loss: 0.036608760194903174\n",
      "Epoch: 122, loss: 0.03660839986964838\n",
      "Epoch: 123, loss: 0.03660804283590328\n",
      "Epoch: 124, loss: 0.03660768906426462\n",
      "Epoch: 125, loss: 0.03660733852560032\n",
      "Epoch: 126, loss: 0.036606991191045296\n",
      "Epoch: 127, loss: 0.03660664703199954\n",
      "Epoch: 128, loss: 0.03660630602012475\n",
      "Epoch: 129, loss: 0.03660596812734139\n",
      "Epoch: 130, loss: 0.03660563332582575\n",
      "Epoch: 131, loss: 0.03660530158800806\n",
      "Epoch: 132, loss: 0.03660497288656817\n",
      "Epoch: 133, loss: 0.03660464719443449\n",
      "Epoch: 134, loss: 0.036604324484780286\n",
      "Epoch: 135, loss: 0.0366040047310211\n",
      "Epoch: 136, loss: 0.0366036879068131\n",
      "Epoch: 137, loss: 0.0366033739860492\n",
      "Epoch: 138, loss: 0.03660306294285764\n",
      "Epoch: 139, loss: 0.036602754751598916\n",
      "Epoch: 140, loss: 0.03660244938686353\n",
      "Epoch: 141, loss: 0.0366021468234697\n",
      "Epoch: 142, loss: 0.03660184703646071\n",
      "Epoch: 143, loss: 0.036601550001102844\n",
      "Epoch: 144, loss: 0.03660125569288305\n",
      "Epoch: 145, loss: 0.03660096408750654\n",
      "Epoch: 146, loss: 0.036600675160894866\n",
      "Epoch: 147, loss: 0.03660038888918322\n",
      "Epoch: 148, loss: 0.03660010524871862\n",
      "Epoch: 149, loss: 0.036599824216058105\n",
      "Epoch: 150, loss: 0.03659954576796572\n",
      "Epoch: 151, loss: 0.03659926988141113\n",
      "Epoch: 152, loss: 0.03659899653356751\n",
      "Epoch: 153, loss: 0.03659872570180931\n",
      "Epoch: 154, loss: 0.036598457363710236\n",
      "Epoch: 155, loss: 0.03659819149704201\n",
      "Epoch: 156, loss: 0.03659792807977103\n",
      "Epoch: 157, loss: 0.036597667090057795\n",
      "Epoch: 158, loss: 0.036597408506254286\n",
      "Epoch: 159, loss: 0.036597152306902705\n",
      "Epoch: 160, loss: 0.03659689847073253\n",
      "Epoch: 161, loss: 0.036596646976660194\n",
      "Epoch: 162, loss: 0.03659639780378607\n",
      "Epoch: 163, loss: 0.03659615093139321\n",
      "Epoch: 164, loss: 0.036595906338945945\n",
      "Epoch: 165, loss: 0.036595664006087485\n",
      "Epoch: 166, loss: 0.036595423912638265\n",
      "Epoch: 167, loss: 0.03659518603859504\n",
      "Epoch: 168, loss: 0.03659495036412862\n",
      "Epoch: 169, loss: 0.03659471686958249\n",
      "Epoch: 170, loss: 0.036594485535470525\n",
      "Epoch: 171, loss: 0.03659425634247657\n",
      "Epoch: 172, loss: 0.03659402927145196\n",
      "Epoch: 173, loss: 0.03659380430341422\n",
      "Epoch: 174, loss: 0.0365935814195456\n",
      "Epoch: 175, loss: 0.03659336060119189\n",
      "Epoch: 176, loss: 0.036593141829860024\n",
      "Epoch: 177, loss: 0.036592925087217724\n",
      "Epoch: 178, loss: 0.03659271035509096\n",
      "Epoch: 179, loss: 0.03659249761546338\n",
      "Epoch: 180, loss: 0.036592286850474484\n",
      "Epoch: 181, loss: 0.03659207804241833\n",
      "Epoch: 182, loss: 0.03659187117374197\n",
      "Epoch: 183, loss: 0.03659166622704407\n",
      "Epoch: 184, loss: 0.036591463185073995\n",
      "Epoch: 185, loss: 0.03659126203073052\n",
      "Epoch: 186, loss: 0.036591062747059075\n",
      "Epoch: 187, loss: 0.03659086531725239\n",
      "Epoch: 188, loss: 0.036590669724648375\n",
      "Epoch: 189, loss: 0.036590475952728026\n",
      "Epoch: 190, loss: 0.03659028398511572\n",
      "Epoch: 191, loss: 0.03659009380557686\n",
      "Epoch: 192, loss: 0.036589905398016924\n",
      "Epoch: 193, loss: 0.03658971874648038\n",
      "Epoch: 194, loss: 0.03658953383514921\n",
      "Epoch: 195, loss: 0.03658935064834208\n",
      "Epoch: 196, loss: 0.036589169170512664\n",
      "Epoch: 197, loss: 0.03658898938624936\n",
      "Epoch: 198, loss: 0.036588811280272714\n",
      "Epoch: 199, loss: 0.03658863483743579\n",
      "Epoch: 200, loss: 0.03658846004272204\n",
      "Epoch: 201, loss: 0.03658828688124444\n",
      "Epoch: 202, loss: 0.036588115338244506\n",
      "Epoch: 203, loss: 0.03658794539909115\n",
      "Epoch: 204, loss: 0.03658777704927949\n",
      "Epoch: 205, loss: 0.03658761027443009\n",
      "Epoch: 206, loss: 0.036587445060287216\n",
      "Epoch: 207, loss: 0.03658728139271855\n",
      "Epoch: 208, loss: 0.03658711925771388\n",
      "Epoch: 209, loss: 0.03658695864138365\n",
      "Epoch: 210, loss: 0.036586799529958476\n",
      "Epoch: 211, loss: 0.036586641909788144\n",
      "Epoch: 212, loss: 0.03658648576734022\n",
      "Epoch: 213, loss: 0.03658633108919908\n",
      "Epoch: 214, loss: 0.03658617786206561\n",
      "Epoch: 215, loss: 0.03658602607275514\n",
      "Epoch: 216, loss: 0.03658587570819758\n",
      "Epoch: 217, loss: 0.03658572675543559\n",
      "Epoch: 218, loss: 0.036585579201624197\n",
      "Epoch: 219, loss: 0.03658543303402951\n",
      "Epoch: 220, loss: 0.03658528824002799\n",
      "Epoch: 221, loss: 0.03658514480710563\n",
      "Epoch: 222, loss: 0.03658500272285668\n",
      "Epoch: 223, loss: 0.03658486197498296\n",
      "Epoch: 224, loss: 0.03658472255129342\n",
      "Epoch: 225, loss: 0.03658458443970207\n",
      "Epoch: 226, loss: 0.036584447628228434\n",
      "Epoch: 227, loss: 0.03658431210499567\n",
      "Epoch: 228, loss: 0.036584177858230524\n",
      "Epoch: 229, loss: 0.03658404487626168\n",
      "Epoch: 230, loss: 0.036583913147519555\n",
      "Epoch: 231, loss: 0.03658378266053517\n",
      "Epoch: 232, loss: 0.036583653403939535\n",
      "Epoch: 233, loss: 0.03658352536646211\n",
      "Epoch: 234, loss: 0.03658339853693122\n",
      "Epoch: 235, loss: 0.0365832729042719\n",
      "Epoch: 236, loss: 0.03658314845750617\n",
      "Epoch: 237, loss: 0.03658302518575178\n",
      "Epoch: 238, loss: 0.03658290307822119\n",
      "Epoch: 239, loss: 0.036582782124221586\n",
      "Epoch: 240, loss: 0.03658266231315295\n",
      "Epoch: 241, loss: 0.03658254363450837\n",
      "Epoch: 242, loss: 0.03658242607787263\n",
      "Epoch: 243, loss: 0.03658230963292167\n",
      "Epoch: 244, loss: 0.0365821942894217\n",
      "Epoch: 245, loss: 0.03658208003722898\n",
      "Epoch: 246, loss: 0.03658196686628826\n",
      "Epoch: 247, loss: 0.03658185476663232\n",
      "Epoch: 248, loss: 0.036581743728382174\n",
      "Epoch: 249, loss: 0.03658163374174483\n",
      "Epoch: 250, loss: 0.03658152479701328\n",
      "Epoch: 251, loss: 0.03658141688456655\n",
      "Epoch: 252, loss: 0.036581309994867424\n",
      "Epoch: 253, loss: 0.036581204118463245\n",
      "Epoch: 254, loss: 0.036581099245984086\n",
      "Epoch: 255, loss: 0.0365809953681429\n",
      "Epoch: 256, loss: 0.036580892475734504\n",
      "Epoch: 257, loss: 0.03658079055963458\n",
      "Epoch: 258, loss: 0.03658068961079972\n",
      "Epoch: 259, loss: 0.0365805896202659\n",
      "Epoch: 260, loss: 0.03658049057914883\n",
      "Epoch: 261, loss: 0.0365803924786424\n",
      "Epoch: 262, loss: 0.03658029531001835\n",
      "Epoch: 263, loss: 0.03658019906462623\n",
      "Epoch: 264, loss: 0.03658010373389146\n",
      "Epoch: 265, loss: 0.03658000930931563\n",
      "Epoch: 266, loss: 0.0365799157824762\n",
      "Epoch: 267, loss: 0.0365798231450248\n",
      "Epoch: 268, loss: 0.03657973138868744\n",
      "Epoch: 269, loss: 0.03657964050526366\n",
      "Epoch: 270, loss: 0.03657955048662565\n",
      "Epoch: 271, loss: 0.03657946132471815\n",
      "Epoch: 272, loss: 0.03657937301155753\n",
      "Epoch: 273, loss: 0.03657928553923111\n",
      "Epoch: 274, loss: 0.036579198899897115\n",
      "Epoch: 275, loss: 0.036579113085783016\n",
      "Epoch: 276, loss: 0.0365790280891864\n",
      "Epoch: 277, loss: 0.03657894390247298\n",
      "Epoch: 278, loss: 0.03657886051807729\n",
      "Epoch: 279, loss: 0.036578777928501185\n",
      "Epoch: 280, loss: 0.036578696126313336\n",
      "Epoch: 281, loss: 0.03657861510414922\n",
      "Epoch: 282, loss: 0.03657853485471031\n",
      "Epoch: 283, loss: 0.03657845537076347\n",
      "Epoch: 284, loss: 0.036578376645140354\n",
      "Epoch: 285, loss: 0.0365782986707368\n",
      "Epoch: 286, loss: 0.0365782214405127\n",
      "Epoch: 287, loss: 0.03657814494749114\n",
      "Epoch: 288, loss: 0.03657806918475791\n",
      "Epoch: 289, loss: 0.03657799414546101\n",
      "Epoch: 290, loss: 0.036577919822810036\n",
      "Epoch: 291, loss: 0.03657784621007558\n",
      "Epoch: 292, loss: 0.036577773300589464\n",
      "Epoch: 293, loss: 0.03657770108774311\n",
      "Epoch: 294, loss: 0.036577629564987835\n",
      "Epoch: 295, loss: 0.03657755872583398\n",
      "Epoch: 296, loss: 0.03657748856385064\n",
      "Epoch: 297, loss: 0.03657741907266482\n",
      "Epoch: 298, loss: 0.036577350245961524\n",
      "Epoch: 299, loss: 0.03657728207748265\n",
      "Epoch: 300, loss: 0.036577214561026784\n",
      "--------------------------\n",
      "ORIGINAL WEIGHTS/BIASES:\n",
      "\n",
      "W1: 1\n",
      "W2: 0.5\n",
      "W3: 1\n",
      "W4: -0.5\n",
      "W5: 1\n",
      "W6: 1\n",
      "B1: 0.5\n",
      "B2: 0\n",
      "B3: 0.5\n",
      "--------------------------\n",
      "FINAL WEIGHTS/BIASES:\n",
      "\n",
      "W1: 1.0514572358400067\n",
      "W2: 0.49999840106287474\n",
      "W3: 1.014988333438993\n",
      "W4: -0.5000006329126121\n",
      "W5: 0.2658051321730554\n",
      "W6: 0.9999995169877434\n",
      "B1: 0.04838695218534944\n",
      "B2: -0.00598235832164403\n",
      "B3: 0.19483435509371008\n"
     ]
    }
   ],
   "source": [
    "# initialize weights and biases\n",
    "w1 = 1\n",
    "w2 = 0.5\n",
    "w3 = 1\n",
    "w4 = -0.5\n",
    "w5 = 1\n",
    "w6 = 1\n",
    "\n",
    "# and the biases\n",
    "bias1 = 0.5\n",
    "bias2 = 0\n",
    "bias3 = 0.5\n",
    "\n",
    "# save the original weights and biases for comparison in the end\n",
    "original_w1 = w1\n",
    "original_w2 = w2\n",
    "original_w3 = w3\n",
    "original_w4 = w4\n",
    "original_w5 = w5\n",
    "original_w6 = w6\n",
    "original_b1 = bias1\n",
    "original_b2 = bias2\n",
    "original_b3 = bias3\n",
    "\n",
    "# learning rate for gradient descent (optimizer)\n",
    "LR = 0.005\n",
    "epochs =  300\n",
    "\n",
    "# replace with generated data (30 rows)\n",
    "data = list(df.values)\n",
    "\n",
    "# let's scale our values with min/max -scaling\n",
    "data = (data - np.min(data)) / (np.max(data) - np.min(data))\n",
    "\n",
    "# points for plotting loss later\n",
    "loss_points = []\n",
    "\n",
    "# train the neural network\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    # make a helper list, that keeps track of loss values in this epoch\n",
    "    epoch_losses = []\n",
    "\n",
    "    for row in data:\n",
    "        # unpack the data into original variables\n",
    "        input1 = row[0]\n",
    "        input2 = row[1]\n",
    "        true_value = row[2]\n",
    "\n",
    "        # FORWARD PASS\n",
    "\n",
    "        # NODE 1 OUTPUT\n",
    "        node_1_output = input1 * w1 + input2 * w3 + bias1\n",
    "        node_1_output = activation_ReLu(node_1_output)\n",
    "        node_1_output\n",
    "\n",
    "        # NODE 2 OUTPUT\n",
    "        node_2_output = input1 * w2 + input2 * w4 + bias2\n",
    "        node_2_output = activation_ReLu(node_2_output)\n",
    "        node_2_output\n",
    "\n",
    "        # NODE 3 OUTPUT \n",
    "        node_3_output = node_1_output * w5 + node_2_output * w6 + bias3\n",
    "        node_3_output = activation_ReLu(node_3_output)\n",
    "        node_3_output\n",
    "\n",
    "        # calculate the loss for this forward pass\n",
    "        predicted_value = node_3_output\n",
    "\n",
    "        # note to self in future\n",
    "        # this will probably crash if the loss value gets too high\n",
    "        # replace with NumPy float64 if needed\n",
    "        loss = (predicted_value - true_value) ** 2\n",
    "        \n",
    "        # add the loss of this data row\n",
    "        # into the current epoch losses\n",
    "        epoch_losses.append(loss)\n",
    "\n",
    "        # BACK PROPAGATION - LAST LAYER\n",
    "\n",
    "        # partial derivative of loss function with respect to w5\n",
    "        # use gradient descent to get updated value for w5\n",
    "        deriv_L_w5 = 2 * node_1_output * (node_1_output * w5 + node_2_output * w6 + bias3 - true_value)\n",
    "        new_w5 = w5 - LR * deriv_L_w5\n",
    "\n",
    "        # partial derivative of loss function with respect to w6\n",
    "        # use gradient descent to get updated value for w6\n",
    "        deriv_L_w6 = 2 * node_2_output * (node_1_output * w5 + node_2_output * w6 + bias3 - true_value)\n",
    "        new_w6 = w6 - LR * deriv_L_w6\n",
    "\n",
    "        # partial derivative of loss function with respect to bias 3\n",
    "        # use gradient descent to get updated value for bias 3\n",
    "        deriv_L_b3 = 2 * 1 * (node_1_output * w5 + node_2_output * w6 + bias3 - true_value)\n",
    "        new_b3 = bias3 - LR * deriv_L_b3\n",
    "\n",
    "        # FROM THIS POINT FORWARD, WE HAVE TO USE THE CHAIN RULE\n",
    "        # IN ORDER TO ACCESS THE NEXT LAYER AFTER THE FINAL LAYER\n",
    "\n",
    "        # chain rule + partial derivations to solve new value for w1\n",
    "        deriv_L_w1_left = 2 * w5 * (node_1_output * w5 + node_2_output * w6 + bias3 - true_value)\n",
    "        deriv_L_w1_right = activation_ReLu_partial_derivative((input1 * w1) + (input2 * w3) + bias1) * input1\n",
    "        deriv_L_w1 = deriv_L_w1_left * deriv_L_w1_right\n",
    "        new_w1 = w1 - LR * deriv_L_w1\n",
    "\n",
    "        # chain rule + partial derivations to solve new value for w2\n",
    "        deriv_L_w2_left = 2 * w6 * (node_1_output * w5 + node_2_output * w6 + bias3 - true_value)\n",
    "        deriv_L_w2_right = activation_ReLu_partial_derivative((input1 * w2) + (input2 * w4) + bias2) * input1\n",
    "        deriv_L_w2 = deriv_L_w2_left * deriv_L_w2_right\n",
    "        new_w2 = w2 - LR * deriv_L_w2\n",
    "\n",
    "        # chain rule + partial derivations to solve new value for w3\n",
    "        deriv_L_w3_left = 2 * w5 * (node_1_output * w5 + node_2_output * w6 + bias3 - true_value)\n",
    "        deriv_L_w3_right = activation_ReLu_partial_derivative((input1 * w1) + (input2 * w3) + bias1) * input2\n",
    "        deriv_L_w3 = deriv_L_w3_left * deriv_L_w3_right\n",
    "        new_w3 = w3 - LR * deriv_L_w3\n",
    "\n",
    "        # chain rule + partial derivations to solve new value for w4\n",
    "        deriv_L_w4_left = 2 * w6 * (node_1_output * w5 + node_2_output * w6 + bias3 - true_value)\n",
    "        deriv_L_w4_right = activation_ReLu_partial_derivative((input1 * w2) + (input2 * w4) + bias2) * input2\n",
    "        deriv_L_w4 = deriv_L_w4_left * deriv_L_w4_right\n",
    "        new_w4 = w4 - LR * deriv_L_w4\n",
    "\n",
    "        # chain rule + partial derivations to solve new value for bias 1\n",
    "        deriv_L_b1_left = 2 * w5 * (node_1_output * w5 + node_2_output * w6 + bias3 - true_value)\n",
    "        deriv_L_b1_right = activation_ReLu_partial_derivative(input1 * w1 + input2 * w3 + bias1) * 1\n",
    "        deriv_L_b1 = deriv_L_b1_left * deriv_L_b1_right\n",
    "        new_b1 = bias1 - LR * deriv_L_b1\n",
    "\n",
    "        # chain rule + partial derivations to solve new value for bias 2\n",
    "        deriv_L_b2_left = 2 * w6 * (node_1_output * w5 + node_2_output * w6 + bias3 - true_value)\n",
    "        deriv_L_b2_right = activation_ReLu_partial_derivative(input1 * w2 + input2 * w4 + bias2) * 1\n",
    "        deriv_L_b2 = deriv_L_b2_left * deriv_L_b2_right\n",
    "        new_b2 = bias2 - LR * deriv_L_b2\n",
    "\n",
    "        # finally replace old weights with the new ones\n",
    "        w1 = new_w1\n",
    "        w2 = new_w2\n",
    "        w3 = new_w3\n",
    "        w4 = new_w4\n",
    "        w5 = new_w5\n",
    "        w6 = new_w6\n",
    "        bias1 = new_b1\n",
    "        bias2 = new_b2\n",
    "        bias3 = new_b3\n",
    "    \n",
    "\n",
    "    # calculate the average loss for WHOLE epoch\n",
    "    average_loss = sum(epoch_losses) / len(epoch_losses)\n",
    "\n",
    "    loss_points.append(average_loss)\n",
    "    print(f\"Epoch: {epoch + 1}, loss: {average_loss}\")\n",
    "    \n",
    "print(\"--------------------------\")\n",
    "print(\"ORIGINAL WEIGHTS/BIASES:\\n\")\n",
    "print(f\"W1: {original_w1}\")\n",
    "print(f\"W2: {original_w2}\")\n",
    "print(f\"W3: {original_w3}\")\n",
    "print(f\"W4: {original_w4}\")\n",
    "print(f\"W5: {original_w5}\")\n",
    "print(f\"W6: {original_w6}\")\n",
    "print(f\"B1: {original_b1}\")\n",
    "print(f\"B2: {original_b2}\")\n",
    "print(f\"B3: {original_b3}\")\n",
    "\n",
    "# IDEA: have a small amount of epochs\n",
    "# but print the current weights after each epoch\n",
    "# can you see a certain development in certain\n",
    "# weights and biases? discuss.\n",
    "print(\"--------------------------\")\n",
    "print(\"FINAL WEIGHTS/BIASES:\\n\")\n",
    "print(f\"W1: {w1}\")\n",
    "print(f\"W2: {w2}\")\n",
    "print(f\"W3: {w3}\")\n",
    "print(f\"W4: {w4}\")\n",
    "print(f\"W5: {w5}\")\n",
    "print(f\"W6: {w6}\")\n",
    "print(f\"B1: {bias1}\")\n",
    "print(f\"B2: {bias2}\")\n",
    "print(f\"B3: {bias3}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the loss for the epochs\n",
    "plt.plot(loss_points)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction is basically just doing the forward \n",
    "# pass again (but only that)\n",
    "def predict(x1, x2):\n",
    "    # NODE 1 OUTPUT\n",
    "    node_1_output = x1 * w1 + x2 * w3 + bias1\n",
    "    node_1_output = activation_ReLu(node_1_output)\n",
    "    node_1_output\n",
    "\n",
    "    # NODE 2 OUTPUT\n",
    "    node_2_output = x1 * w2 + x2 * w4 + bias2\n",
    "    node_2_output = activation_ReLu(node_2_output)\n",
    "    node_2_output\n",
    "\n",
    "    # NODE 3 OUTPUT \n",
    "    node_3_output = node_1_output * w5 + node_2_output * w6 + bias3\n",
    "    node_3_output = activation_ReLu(node_3_output)\n",
    "    node_3_output\n",
    "\n",
    "    return node_3_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>bmi</th>\n",
       "      <th>charges</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19</td>\n",
       "      <td>27.9</td>\n",
       "      <td>16884.924</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age   bmi    charges\n",
       "0   19  27.9  16884.924"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get a test row in original values:\n",
    "df.iloc[0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4.76829326e-05, 1.87280992e-04, 2.64592656e-01])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the same row in scaled version\n",
    "# you can google for decimal formats, for example:\n",
    "# \"4.76829326e-05 in decimal format\"\n",
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x1 = 0.00004768293 (19 years of age)\n",
    "# x2 = 0.00018728099 (27.9 BMI)\n",
    "# y/target = 0.264592656 (16884.924)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.20775970824147805"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# output with scaled values is this\n",
    "# if you multiply this with the MAX value of charges, \n",
    "# you get USD value\n",
    "predict(0.00004768293, 0.00018728099)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13248.92551779178"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the USD value\n",
    "df['charges'].max() * predict(0.00004768293, 0.00018728099)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>bmi</th>\n",
       "      <th>charges</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>age</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.113048</td>\n",
       "      <td>0.298624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bmi</th>\n",
       "      <td>0.113048</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.199846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>charges</th>\n",
       "      <td>0.298624</td>\n",
       "      <td>0.199846</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              age       bmi   charges\n",
       "age      1.000000  0.113048  0.298624\n",
       "bmi      0.113048  1.000000  0.199846\n",
       "charges  0.298624  0.199846  1.000000"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.corr()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
